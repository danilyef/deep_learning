{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:rgb(0,120,170)\">Numpy RNN</h1>\n",
    "\n",
    "\n",
    "In the notebook, I will implement the recurrent neural network (RNN) using only NumPy functionality. Concretely, I will implement implement the forward and backward pass through the RNN and validate the implementation doing numerical gradient checking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python version: 3.9.12\n",
      "numpy version: 1.21.5\n"
     ]
    }
   ],
   "source": [
    "from typing import Dict, Tuple\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "print('python version: 3.9.12')\n",
    "print('numpy version:',np.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:rgb(0,120,170)\">Task 1: Implement Recurrent Neural Network </h2>\n",
    "\n",
    "In this task you I will implement a recurrent neural network (RNN) in a sequence-to-one setting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forward equations of the RNN:\n",
    "$$\n",
    "        \\boldsymbol{s}(t) = \\boldsymbol{W}^T \\boldsymbol{x}(t) + \\boldsymbol{R}^T \\boldsymbol{a}(t-1) + \\boldsymbol{b}_s\\\\\n",
    "        \\boldsymbol{a}(t) = \\operatorname{tanh}(\\boldsymbol{s}(t)) \\\\\n",
    "        \\hat{\\boldsymbol{y}}(t) = \\boldsymbol{V}^T \\boldsymbol{a}(t) + \\boldsymbol{b}_y\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(object):\n",
    "    \"\"\"Numpy implementation of sequence-to-one recurrent neural network for regression tasks.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_size: int, hidden_size: int, output_size: int):\n",
    "\n",
    "        super(RNN, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "\n",
    "        # create and initialize weights of the network\n",
    "        \n",
    "        self.W = np.random.uniform(-0.2,0.2,size = (self.input_size,self.hidden_size))\n",
    "        self.R = np.random.uniform(-0.2,0.2,size = (self.hidden_size,self.hidden_size))\n",
    "        self.bs = np.zeros((self.hidden_size,1))\n",
    "        self.V = np.random.uniform(-0.2,0.2,size = (self.hidden_size,self.output_size))\n",
    "        self.by = np.zeros((self.output_size,1))\n",
    "        self.reset_parameters()\n",
    "\n",
    "        # place holder to store intermediates for backprop\n",
    "        self.a = {0:np.zeros((self.hidden_size,1))}\n",
    "        self.y_hat = None\n",
    "        self.grads = None\n",
    "        self.x = None\n",
    "\n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "\n",
    "        self.x = np.expand_dims(x,axis = 2)\n",
    "        for i in range(len(x)):\n",
    "            s_t = self.W.T @ self.x[i] + self.R.T @ self.a[i] + self.bs\n",
    "            a_t = np.tanh(s_t)\n",
    "            self.a[i + 1] = a_t\n",
    "            \n",
    "        \n",
    "        self.y_hat = self.V.T @ a_t + self.by\n",
    "        \n",
    "            \n",
    "        return self.y_hat\n",
    "\n",
    "    def backward(self, d_loss: np.ndarray) -> Dict:\n",
    "      \n",
    "        delta_t = np.zeros((1,self.hidden_size))\n",
    "        \n",
    "        dV = np.zeros(self.V.T.shape)\n",
    "        dby = np.zeros((self.output_size,1)).T\n",
    "        \n",
    "        dW = np.zeros(self.W.T.shape)\n",
    "        dR = np.zeros(self.R.T.shape)\n",
    "        dbs = np.zeros(self.bs.T.shape)\n",
    "        \n",
    "        dV = d_loss @ self.a[len(self.a)-1].T\n",
    "        dby += d_loss.T\n",
    "\n",
    "        for i in reversed(range(1,len(self.a))):\n",
    "            \n",
    "            if i == len(self.a) - 1:\n",
    "                delta_t = (d_loss.T @ self.V.T + delta_t @ self.R.T) @ np.diag(np.diag(1 - self.a[i] @ self.a[i].T))\n",
    "            else:\n",
    "                delta_t = (delta_t @ self.R.T) @ np.diag(np.diag(1 - self.a[i] @ self.a[i].T))\n",
    "        \n",
    "            dW += delta_t.T @ self.x[i-1].T\n",
    "            dR += delta_t.T @ self.a[i-1].T\n",
    "            dbs += delta_t\n",
    "            \n",
    "        self.grads = {'dW': dW, 'dR': dR, 'dV': dV, 'dbs': dbs, 'dby': dby}\n",
    "        return self.grads\n",
    "        \n",
    "        \n",
    "    def update(self, lr: float):\n",
    "\n",
    "        if not self.grads:\n",
    "            raise RuntimeError(\"You have to call the .backward() function first\")\n",
    "            \n",
    "\n",
    "        self.V -= lr * self.grads['dV'].T\n",
    "        self.W -= lr * self.grads['dW'].T\n",
    "        self.R -= lr * self.grads['dR'].T\n",
    "        self.bs -= lr * self.grads['dbs'].T\n",
    "        self.by -= lr * self.grads['dby'].T\n",
    "        # reset internal class attributes\n",
    "        self.grads = {}\n",
    "        self.y_hat, self.a = None, {0:np.zeros((self.hidden_size,1))}\n",
    "        \n",
    "    def get_weights(self) -> Dict:\n",
    "        return {'W': self.W, 'R': self.R, 'V': self.V, 'bs': self.bs, 'by': self.by}\n",
    "    \n",
    "    def set_weights(self, weights: Dict):\n",
    "        if not all(name in weights.keys() for name in ['W', 'R', 'V']):\n",
    "            raise ValueError(\"Missing one of 'W', 'R', 'V' keys in the weight dictionary\")\n",
    "        self.W = weights[\"W\"]\n",
    "        self.R = weights[\"R\"]\n",
    "        self.V = weights[\"V\"]\n",
    "        self.bs = weights[\"bs\"]\n",
    "        self.by = weights[\"by\"]\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        # recurrent weights initialized as identity matrix\n",
    "        self.R = np.eye(self.hidden_size)\n",
    "        \n",
    "        # input to hidden and hidden to output initialized with LeCun initialization\n",
    "        gain = np.sqrt(3 / self.input_size)\n",
    "        self.W = np.random.uniform(-gain, gain, self.W.shape)\n",
    "        gain = np.sqrt(3 / self.hidden_size)\n",
    "        self.V = np.random.uniform(-gain, gain, self.V.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:rgb(0,120,170)\">Task 2: Numerical gradient check</h2>\n",
    "\n",
    "To validate your implementation of the backward pass use the two-sided gradient approximation\n",
    "\n",
    "$$\n",
    "\\frac{\\partial f(w_1, \\dots, w_n)}{\\partial w_i} \\approx \\frac{f(w_1, \\dots, w_i + \\epsilon, \\dots, w_n) - f(w_1, \\dots, w_i - \\epsilon, \\dots, w_n)}{2*\\epsilon}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_numerical_gradient(model: RNN, x: np.ndarray, eps: float=1e-7) -> Dict:\n",
    "    \"\"\"Implementation of the two-sided numerical gradient approximation\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model : RNN\n",
    "        The RNN model object\n",
    "    x : np.ndarray\n",
    "        Input sequence(s) of shape [sequence length, number of features]\n",
    "    eps : float\n",
    "        The epsilon used for numerical gradient approximation\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    A dictionary containing the numerical gradients for each weight of the RNN. Make sure\n",
    "    to name the dictionary keys like the names of the RNN gradients dictionary (e.g. \n",
    "    'd_R' for the weight 'R')\n",
    "    \"\"\"  \n",
    "    weights = model.get_weights()\n",
    "    num_grad = {}\n",
    "    \n",
    "    for name_layer,weights_layer in weights.items():#iterate through each layer\n",
    "        gradient = np.zeros((weights_layer.shape))\n",
    "        for i in range(weights_layer.shape[0]):\n",
    "            for j in range(weights_layer.shape[1]):\n",
    "                weights_plus = weights.copy()\n",
    "                weights_min = weights.copy()\n",
    "                \n",
    "                #changing one element in weight layer:\n",
    "                w_min = np.copy(weights_layer)\n",
    "                w_min[i,j] = w_min[i,j] - eps\n",
    "\n",
    "                w_plus = np.copy(weights_layer)\n",
    "                w_plus[i,j] = w_plus[i,j] + eps; \n",
    "                \n",
    "                #computing forward passes for plus and minus epsilons\n",
    "                weights_min[name_layer] = w_min\n",
    "                model.set_weights(weights_min)\n",
    "                forward_min = model.forward(x)\n",
    "                \n",
    "                weights_plus[name_layer] = w_plus\n",
    "                model.set_weights(weights_plus)\n",
    "                forward_plus = model.forward(x)\n",
    "                \n",
    "                gradient[i,j] = np.sum((forward_plus - forward_min)/(2*eps))\n",
    "         \n",
    "                \n",
    "        \n",
    "        num_grad['d' + name_layer] = gradient.T        \n",
    "    return num_grad\n",
    "\n",
    "\n",
    "def get_analytical_gradient(model: RNN, x: np.ndarray) -> Dict:\n",
    "    \"\"\"Helper function to get the analytical gradient.\n",
    "        \n",
    "    Parameters\n",
    "    ----------\n",
    "    model : RNN\n",
    "        The RNN model object\n",
    "    x : np.ndarray\n",
    "        Input sequence(s) of shape [sequence length, number of features]\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    A dictionary containing the analytical gradients for each weight of the RNN.\n",
    "    \"\"\"     \n",
    "    e_t = np.ones((model.output_size,1))\n",
    "        \n",
    "    res = model.forward(x)\n",
    "    grads = model.backward(e_t)\n",
    "    return grads\n",
    "\n",
    "            \n",
    "def gradient_check(model: RNN, x: np.ndarray, treshold: float = 1e-7):\n",
    "    \"\"\"Perform gradient checking.\n",
    "    \n",
    "    You don't have to do anything in this function.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model : RNN\n",
    "        The RNN model object\n",
    "    x : np.ndarray\n",
    "        Input sequence(s) of shape [sequence length, number of features]\n",
    "    eps : float\n",
    "        The epsilon used for numerical gradient approximation    \n",
    "    \"\"\"\n",
    "    numerical_grads = get_numerical_gradient(model, x)\n",
    "    analytical_grads = get_analytical_gradient(model, x)\n",
    "    \n",
    "    for key, num_grad in numerical_grads.items():\n",
    "        difference = np.linalg.norm(num_grad - analytical_grads[key])\n",
    "        if difference < treshold:\n",
    "            print(f\"Gradient check for {key} passed (difference {difference:.3e})\")\n",
    "        else:\n",
    "            print(f\"Gradient check for {key} failed (difference {difference:.3e})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we use the `gradient_check` function to test your RNN implementation for two different scenarios, which are a single output neuron and multiple output neurons. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check with a single output neuron:\n",
      "Gradient check for dW passed (difference 2.035e-09)\n",
      "Gradient check for dR passed (difference 2.565e-09)\n",
      "Gradient check for dV passed (difference 1.133e-09)\n",
      "Gradient check for dbs passed (difference 8.493e-10)\n",
      "Gradient check for dby passed (difference 2.876e-11)\n",
      "\n",
      "Gradient check with multiple output neurons:\n",
      "Gradient check for dW passed (difference 4.929e-09)\n",
      "Gradient check for dR passed (difference 6.825e-09)\n",
      "Gradient check for dV passed (difference 2.638e-09)\n",
      "Gradient check for dbs passed (difference 1.614e-09)\n",
      "Gradient check for dby passed (difference 7.877e-10)\n"
     ]
    }
   ],
   "source": [
    "print(\"Gradient check with a single output neuron:\")\n",
    "output_size = 1\n",
    "model = RNN(input_size=5, hidden_size=10, output_size=output_size)\n",
    "x = np.random.rand(5, 5)\n",
    "gradient_check(model, x)\n",
    "\n",
    "print(\"\\nGradient check with multiple output neurons:\")\n",
    "output_size = 5\n",
    "model = RNN(input_size=5, hidden_size=10, output_size=output_size)\n",
    "x = np.random.rand(5, 5)\n",
    "gradient_check(model, x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
