{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vision Networks and Fast Training: Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will take a closer look at some famous vision architectures.\n",
    "Since most of these architectures are very large, it requires high-end hardware to train from scratch.\n",
    "To leverage the limited availability of hardware, also *Transfer Learning* can be used. \n",
    "By using stored weights of a large network a new network can be trained cheaply on new datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.9.12\n",
      "Pytorch version: 1.13.0\n",
      "torchvision version: 0.14.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.notebook import trange\n",
    "\n",
    "torch.manual_seed(1806)\n",
    "torch.cuda.manual_seed(1806)\n",
    "\n",
    "print('Python version: 3.9.12')\n",
    "print('Pytorch version:', torch.__version__)\n",
    "print('torchvision version:', torchvision.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## LeNet-5 and its Offspring\n",
    "\n",
    "![LeNet-5 architecture](https://miro.medium.com/max/2154/1*1TI1aGBZ4dybR6__DI9dzA.png)\n",
    "\n",
    "The LeNet-5 architecture (depicted above) is one of the first convolutional networks.\n",
    "Since convolutions are extremely well suited for many computer vision tasks,\n",
    "a wide variety of network architectures using convolutional layers has become available.\n",
    "Although the differences in performance are sometimes large,\n",
    "the architectures can generally be considered variations on the same theme."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "###### Alexnet\n",
    "\n",
    "![alex-net architecture](https://cdn-images-1.medium.com/max/1000/1*wzflNwJw9QkjWWvTosXhNw.png)\n",
    "\n",
    "In 2012 Alex Krizhevsky et al. won the [Imagenet Large Scale Visual Recognition Challenge](http://www.image-net.org/challenges/LSVRC/) (ILSVRC).\n",
    "The network they used, which is known as *Alex-net*, is depicted below and follows the same basic principles as LeNet-5.\n",
    "Alex-net has quite a bit more parameters than LeNet-5, therefore it requires a large amount of computational resources to train.\n",
    "\n",
    "To speed up training time, Alex-net was trained on GPU.\n",
    "Since GPUs have access to little memory compared to CPUs (especially back in the days),\n",
    "alex-net did not fit on a single GPU and required 2 GPUs to train the model,\n",
    "hence the distinction between two paths in the illustration of the network.\n",
    "\n",
    "On modern GPUs, it is no longer a problem to fit alex-net on a single GPU.\n",
    "Due to the fact that deep learning frameworks mostly support hardware acceleration, \n",
    "it has even become extremely easy and almost common to train (large) networks on GPUs.\n",
    "A more detailed description on how to achieve this in pytorch, is given below.\n",
    "\n",
    "Another important add-on, is the use of the dropout regularisation technique in the fully connected layers.\n",
    "From DL & NN 1 you should remember that dropout behaves differently during testing and training.\n",
    "When using Dropout or other modules with different behaviour, e.g. BatchNorm, in pytorch, \n",
    "it is important to make sure that your network operates in the right mode.\n",
    "To do this, the `nn.Module` class provides the `train` and `eval` methods\n",
    "and invokes it on all submodules to assure that the desired behaviour is triggered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Pytorch GPU acceleration\n",
    "\n",
    "In pytorch, training a model on GPU is relatively easy.\n",
    "To copy a tensor `x` from main memory (or wherever it may be) to GPU memory,\n",
    "all we need to do is call `x.to('cuda')` or equivalently `x.cuda()`.\n",
    "When multiple GPUs are available, `x.to('cuda:0')` copies a tensor to the first GPU,\n",
    "`x.to('cuda:1')` to the second, etc.\n",
    "Similarly, to copy a tensor from a GPU (or again wherever it may be) to main memory,\n",
    "`x.to('cpu')` or equivalently `x.cpu()` can be used.\n",
    "\n",
    "Whenever a computation is done on tensors that reside on a specific device,\n",
    "the result will also be on that device.\n",
    "It is not possible, however, to make computations with tensors from different devices.\n",
    "This means that the training of an entire network automatically takes place on e.g. a GPU,\n",
    "as soon as all the variables reside on the same device.\n",
    "When working with neural networks, \n",
    "this is the case if both the network parameters and the data are moved to the same device.\n",
    "\n",
    "To move all parameters of a network to the correct device,\n",
    "The `nn.Module` class provides a convenience `to` method \n",
    "that moves all registered parameters, buffers and submodules to the correct device.\n",
    "\n",
    "As for the data, it is often possible to fit the entire dataset in GPU memory.\n",
    "However, often it does not provide any advantages or it even comes with disadvantages.\n",
    "E.g. the `MNIST` dataset from `torchvision` provides PIL images that can not reside on GPU.\n",
    "If the dataset would be stored on the GPU, the data would have to move to CPU first,\n",
    "where the pre-processing is done on the PIL images, and then move back to the GPU.\n",
    "Therefore, it is considered good practice to keep the dataset in main memory\n",
    "and move the samples to the GPU only when they are needed for computation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Hardware Acceleration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(network: nn.Module, data: DataLoader, metric: callable) -> list:\n",
    "    \"\"\"\n",
    "    Evaluate the performance of a network on some metric.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    network : nn.Module\n",
    "        Pytorch module representing the network.\n",
    "    data : DataLoader\n",
    "        Pytorch dataloader that is able to \n",
    "        efficiently sample mini-batches of data.\n",
    "    metric : callable\n",
    "        Function that computes a scalar metric\n",
    "        from the network logits and true data labels.\n",
    "        The function should expect pytorch tensors as inputs.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    errors : list\n",
    "        The computed metric for each mini-batch in `data`.\n",
    "    \"\"\"\n",
    "    \n",
    "    errors_batch = []  \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    network.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, (x,y) in enumerate(data):\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            \n",
    "\n",
    "            pred = network.forward(x)\n",
    "            res = metric(pred,y)\n",
    "            errors_batch.append(res)\n",
    "    \n",
    "    return errors_batch\n",
    "    \n",
    "\n",
    "@torch.enable_grad()\n",
    "def update(network: nn.Module, data: DataLoader, loss: nn.Module, \n",
    "           opt: optim.Optimizer) -> list:\n",
    "    \"\"\"\n",
    "    Update the network to minimise some loss using a given optimiser.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    network : nn.Module\n",
    "        Pytorch module representing the network.\n",
    "    data : DataLoader\n",
    "        Pytorch dataloader that is able to \n",
    "        efficiently sample mini-batches of data.\n",
    "    loss : nn.Module\n",
    "        Pytorch function that computes a scalar loss\n",
    "        from the network logits and true data labels.\n",
    "    opt : optim.Optimiser\n",
    "        Pytorch optimiser to use for minimising the objective.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    errors : list\n",
    "        The computed loss for each mini-batch in `data`.\n",
    "    \"\"\" \n",
    "    loss_batch = []\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    " \n",
    "    network.train()\n",
    "    for i, (x,y) in enumerate(data):\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        opt.zero_grad()\n",
    "\n",
    "        pred = network.forward(x)\n",
    "        loss_res = loss(pred,y)\n",
    "        loss_res.backward()\n",
    "        opt.step()\n",
    "\n",
    "        loss_batch.append(loss_res.item())\n",
    "        \n",
    "    return loss_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "###### VGG\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/2628/1*lZTWFT36PXsZZK3HjZ3jFQ.png\" \n",
    "     alt=\"VGG architecture\" style=\"width: 70%; margin: auto\" />\n",
    "\n",
    "The Visual Geometry Group at Oxford University introduced \n",
    "different versions of architectures that are now known as VGG net.\n",
    "11-layer, 16-layer and 19-layer variants exist,\n",
    "all of which use only 3x3 convolutions in the feature extraction part.\n",
    "\n",
    "After winning the Imagenet Large Scale Visual Recognition Challenge (ILSVRC) in 2014,\n",
    "the weights of the winning models were made [available](https://www.robots.ox.ac.uk/~vgg/research/very_deep/).\n",
    "This made it possible for researchers with lower computational budgets\n",
    "to make use of the features the network has extracted for natural images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Task 2: VGG for CIFAR-10\n",
    "\n",
    "Inception has been trained on the ImageNet dataset, which is hard to come by:\n",
    "it is very large (a few 100GB) and requires registration to get access to the images.\n",
    "[CIFAR-10 and CIFAR-100](https://www.cs.toronto.edu/~kriz/cifar.html)\n",
    "are similar datasets that are much easier to obtain\n",
    "and are also one of the standard datasets in `torchvision.datasets`.\n",
    "In this exercise the goal is to build the VGG model for CIFAR-10\n",
    "so that it uses the same feature extraction architecture as `torchvision.models.VGG`.\n",
    "\n",
    " > Create a network with the exact same convolutional layers as\n",
    " > `torchvision.models.VGG` so that it can be used for CIFAR images.\n",
    " > Concretely, the goal is to replace the classifier to predict CIFAR labels\n",
    " > instead of the Imagenet labels.\n",
    " > Use global average pooling to make the classifier independent of the exact image size.\n",
    " > Keep the classifier architecture rectangular, i.e. same width for all layers (except for the classes).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class CifarVGG(nn.Module):\n",
    "    \"\"\" Variant of the VGG network for classifying CIFAR images. \"\"\"\n",
    "    \n",
    "    def __init__(self, features: nn.Module, num_classes: int = 10):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        features : nn.Module\n",
    "            The convolutional part of the VGG network.\n",
    "        num_classes : int\n",
    "            The number of output classes in the data.\n",
    "        \"\"\"\n",
    "        super(CifarVGG,self).__init__()\n",
    "        self.features = features\n",
    "        self.features.global_average = torch.nn.AdaptiveAvgPool2d((1,1))\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512,512),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(512, num_classes),\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.features(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Task 3: Existing Features \n",
    "\n",
    "Training a network like VGG (or any of the other networks in this assignment)\n",
    "can take a few hours when training on a GPU.\n",
    "Therefore it is often useful to be able to load pre-trained weights into the network.\n",
    "Also, saving a model that has been trained for hours can often save a lot of time.\n",
    "In pytorch this is possible through what is called \n",
    "[`state_dict`s](https://pytorch.org/tutorials/beginner/saving_loading_models.html).\n",
    "Saving the parameters of a pytorch module can be done with `torch.save(module.state_dict(), path)`,\n",
    "whereas loading saved parameters is done with `module.load_state_dict(torch.load(path))`.\n",
    "\n",
    " > Write a function `vgg_init_` to initialise a `CifarVGG` network.\n",
    " > It should load the pre-trained weights for the **13-layer variant of VGG** from `torchvision.models.vgg`\n",
    " > to initialise the feature extractor of the model\n",
    " > and reasonably initialise the classifier using initialisation functions from `torch.nn.init`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def vgg_init_(network: CifarVGG):\n",
    "    \"\"\"\n",
    "    Initialise a CifarVGG network with a pre-trained VGG feature extractor.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    network : CifarVGG\n",
    "        The model to initialise.\n",
    "    \"\"\"\n",
    "    #from torchvision.models import vgg\n",
    "    weights = torch.hub.load_state_dict_from_url('https://download.pytorch.org/models/vgg13-c768596a.pth')\n",
    "    \n",
    "    #assigning weights of feature:\n",
    "    for layer in network.state_dict().keys():\n",
    "        index = int(layer.split('.')[1])\n",
    "        \n",
    "        if layer.startswith('features') and 'weight' in layer:\n",
    "            network.features[index].weight = nn.Parameter(weights[layer])           \n",
    "            \n",
    "        elif layer.startswith('features') and 'bias' in layer:  \n",
    "            network.features[index].bias = nn.Parameter(weights[layer])\n",
    "            \n",
    "            \n",
    "        elif layer.startswith('classifier') and 'weight' in layer:\n",
    "            torch.nn.init.xavier_uniform_(network.state_dict()[layer],gain=nn.init.calculate_gain('relu'))\n",
    "            \n",
    "        elif layer.startswith('classifier') and 'bias' in layer:\n",
    "            torch.nn.init.zeros_(network.state_dict()[layer])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/vgg13-c768596a.pth\" to /Users/daniil.yefimov/.cache/torch/hub/checkpoints/vgg13-c768596a.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ad8a5e0341e403fb383e0f916583287",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/508M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# sanity check\n",
    "vgg13 = torchvision.models.vgg13()\n",
    "network = CifarVGG(vgg13.features, num_classes=10)\n",
    "vgg_init_(network)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Task 4: Training (part of) the Network \n",
    "\n",
    "Obviously, a classifier for CIFAR 10 will be different from a classifier for Imagenet.\n",
    "With the initialisation above, the `CifarVGG` has a ready-to-go feature extractor,\n",
    "but the classifier part still has to be trained.\n",
    "To do this training efficiently, there are a few things left to do.\n",
    "\n",
    " > The code below should train the entire network on the entire dataset for a few epochs.\n",
    " > To do:\n",
    " > 1. Only trains the classifier part of the network\n",
    " > and leaves the convolutional feature extractor untouched, i.e., *frozen*. \n",
    " > 2. The 32x32 CIFAR images are upscaled to 224x224 pixels. \n",
    " > 3. Training is done on the GPU, which is generally faster. \n",
    " > Also, use a parallel dataloader to make sure that the GPU does not have to wait for data.\n",
    " > 4. Only a subset of 500 images from the CIFAR data is used for training. \n",
    " > 5. A subset of 500 images from the CIFAR data is used as validation data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28bde7f93ed943758d994cadec96dcc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# set up data\n",
    "normalise=torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Resize((224,224)),\n",
    "    torchvision.transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))\n",
    "    \n",
    "])\n",
    "cifar10_train = torchvision.datasets.CIFAR10(\"~/.pytorch\", transform = normalise, download = True,train = True)\n",
    "\n",
    "subset_train = torch.utils.data.Subset(cifar10_train, list(range(0,500)))\n",
    "subset_val = torch.utils.data.Subset(cifar10_train, list(range(500,1001)))\n",
    "\n",
    "samples_train = DataLoader(subset_train, batch_size=32,num_workers = 2,pin_memory=True,shuffle = True)\n",
    "samples_val = DataLoader(subset_val, batch_size=32,num_workers = 2,pin_memory=True,shuffle = True)\n",
    "\n",
    "\n",
    "\n",
    "# create and initialise model\n",
    "vgg13 = torchvision.models.vgg13()\n",
    "network = CifarVGG(vgg13.features, num_classes=10)\n",
    "vgg_init_(network)\n",
    "\n",
    "#Freeze Convolutional layers\n",
    "for param in network.features.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "#Move training to GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "network = network.to(device)\n",
    "\n",
    "# optimiser + loss function\n",
    "sgd = optim.SGD(network.parameters(), lr=1e-1)\n",
    "ce = nn.CrossEntropyLoss()\n",
    "\n",
    "# train for 20 epochs\n",
    "train_errs, valid_errs = [], []\n",
    "for _ in trange(20):\n",
    "    \n",
    "    #training\n",
    "    local_errs_train = update(network, samples_train, ce, sgd)\n",
    "    train_errs.append(sum(local_errs_train) / len(local_errs_train))\n",
    "    \n",
    "    #validation\n",
    "    local_errs_val = evaluate(network, samples_val, ce)\n",
    "    valid_errs.append(sum(local_errs_val) / len(local_errs_val))\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot learning curves\n",
    "from matplotlib import pyplot as plt\n",
    "plt.plot(train_errs, label=\"train\")\n",
    "plt.plot(valid_errs, label=\"valid\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5: Accuracy\n",
    "\n",
    "In order to get an idea of how well this transfer learning worked,\n",
    "we can evaluate the accuracy on the CIFAR 10 test data.\n",
    "\n",
    " > Compute and output the accuracy on the training, validation and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def accuracy(logits, targets):\n",
    "    \"\"\"\n",
    "    Compute the accuracy for given logits and targets.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    logits : (N, K) torch.Tensor\n",
    "        A mini-batch of logit vectors from the network.\n",
    "    targets : (N, ) torch.Tensor\n",
    "        A mini_batch of target scalars representing the labels.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    acc : () torch.Tensor\n",
    "        The accuracy over the mini-batch of samples.\n",
    "    \"\"\"\n",
    "    value,ind_pred = torch.max(logits.data,1)\n",
    "    return (ind_pred == targets).sum().item()/len(targets)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "cifar10_test = torchvision.datasets.CIFAR10(\"~/.pytorch\", transform = normalise, download = True,train = False)\n",
    "samples_test = DataLoader(cifar10_test, batch_size=32,num_workers = 2,pin_memory=True,shuffle = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6642372204472844"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_acc = []\n",
    "for i, (x,y) in enumerate(samples_test):\n",
    "    x = x.to(device)\n",
    "    y = y.to(device)\n",
    "    pred = network.forward(x)\n",
    "    res = accuracy(pred,y)\n",
    "    test_acc.append(res)\n",
    "    \n",
    "sum(test_acc)/len(test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Skip-connections\n",
    "\n",
    "One of the most popular modern network architectures for vision is the residual network.\n",
    "The main feature of this architecture is the so-called skip-connection,\n",
    "which allows to combine the activations with the original inputs in each layer.\n",
    "Since these skip-connections open up a gradient highway,\n",
    "they make it possible to train much deeper networks than is possible without the skip-connections.\n",
    "\n",
    "Mathematically, the simplest form of a skip connection can be written as\n",
    "$$\\boldsymbol{s} = \\boldsymbol{x} + f(\\boldsymbol{x}).$$\n",
    "In order for this to work, the dimensions of $\\boldsymbol{x}$ and $f(\\boldsymbol{x})$ must line up.\n",
    "This means that in this formulation, only square layers,\n",
    "i.e. layers with the same number of inputs and outputs, are possible.\n",
    "\n",
    "In order to use a skip-connection on layers that reduce the dimensionality,\n",
    "a linear transform on $\\boldsymbol{x}$ can be inserted in the equation.\n",
    "Since also other operations are possible, on both inputs and (pre-)activations, \n",
    "we can generalise the skip-connection formula to\n",
    "$$\\boldsymbol{s} = \\boldsymbol{C} \\cdot \\boldsymbol{x} + \\boldsymbol{T} \\cdot f(\\boldsymbol{x}),$$\n",
    "where $\\boldsymbol{C}$ and $\\boldsymbol{T}$ are linear transformations (a.k.a. matrices)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Task 6: Highway Networks\n",
    "\n",
    "Given the general skip-connection formula above,\n",
    "residual networks can be thought of as a special case of highway networks.\n",
    "The layers in a highway network use the following expression to build the gradient highway:\n",
    "$$\\boldsymbol{a} = c(\\boldsymbol{x}) \\odot \\boldsymbol{x} + t(\\boldsymbol{x}) \\odot f(\\boldsymbol{x}),$$\n",
    "where $f$ is the regular transform that computes activations from the inputs\n",
    "and $c$ and $t$ are gates, i.e. layers that have activations in $[0, 1]$ (cf. LSTMs). \n",
    "The gates control how much of the inputs, resp. outputs is to be used.\n",
    "Especially interesting is the case where $c(\\boldsymbol{x}) = 1 - t(\\boldsymbol{x})$, \n",
    "since this forces each neuron in the model to *choose* between inputs and activations and requires less weights.\n",
    "\n",
    " > Implement the `HighwayLayer` class to represent a layer in a highway network.\n",
    " > Use fully connected layers with reasonable activation functions for $f$, $t$ and $c$.\n",
    " > Make sure that it is possible to construct non-square layers and to use all keywordarguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class HighwayLayer(nn.Module):\n",
    "    \"\"\" Fully connected layer in a highway network. \"\"\"\n",
    "    \n",
    "    def __init__(self, in_features: int, out_features: int,\n",
    "                 carry: bool = False, act:callable = torch.tanh):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        in_features : int\n",
    "            Number of input dimensions.\n",
    "        out_features : int\n",
    "            Number of output dimensions.\n",
    "        carry : bool\n",
    "            Whether or not to use a separate carry gate.\n",
    "        act : callable\n",
    "            Activation function to use.\n",
    "        \"\"\"\n",
    "        super(HighwayLayer,self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        \n",
    "        \n",
    "        self.linear_layer_f = nn.Linear(in_features,out_features)\n",
    "        self.linear_layer_c = nn.Linear(in_features,out_features)\n",
    "        self.linear_layer_t =  nn.Linear(in_features,out_features)\n",
    "        \n",
    "        self.carry = carry\n",
    "        \n",
    "        self.act = act\n",
    "        self.act_c = torch.sigmoid\n",
    "        self.act_t = torch.sigmoid\n",
    "        \n",
    "        #Change dimension in order to add the input\n",
    "        self.identity_tens = nn.Linear(in_features,out_features,bias = False)\n",
    "        if self.in_features == self.out_features:\n",
    "            self.identity_tens.requires_grad = False\n",
    "        \n",
    "    def forward(self,x):\n",
    "        if self.carry:\n",
    "            f_layer = self.linear_layer_f(x)\n",
    "            f_layer = self.act(f_layer)\n",
    "            \n",
    "            c_layer = self.linear_layer_c(x)\n",
    "            c_layer = self.act_c(c_layer)\n",
    "            \n",
    "            t_layer = self.linear_layer_t(x)\n",
    "            t_layer = self.act_t(t_layer)\n",
    "            \n",
    "            if self.in_features != self.out_features:\n",
    "                x = self.identity_tens(x)\n",
    "                \n",
    "            \n",
    "                    \n",
    "            res = c_layer * x + t_layer * f_layer\n",
    "            return res\n",
    "            \n",
    "        else:\n",
    "            f_layer = self.linear_layer_f(x)\n",
    "            f_layer = self.act(f_layer)\n",
    "            \n",
    "            t_layer = self.linear_layer_t(x)\n",
    "            t_layer = self.act_t(t_layer)\n",
    "                   \n",
    "            c_layer = 1 - t_layer\n",
    "            \n",
    "            if self.in_features != self.out_features:\n",
    "                x = self.identity_tens(x) \n",
    "            \n",
    "            res = c_layer * x + t_layer * f_layer\n",
    "            return res\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40300"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sanity check 1\n",
    "highway = HighwayLayer(100, 100, carry = False, act = torch.tanh)\n",
    "highway(torch.randn(1, 100))\n",
    "sum([par.numel() for par in highway.parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40030"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sanity check 2\n",
    "highway = HighwayLayer(1000, 10,carry = False, act = torch.tanh)\n",
    "highway(torch.randn(1, 1000))\n",
    "sum([par.numel() for par in highway.parameters()])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
