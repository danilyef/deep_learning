{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Auto-Encoders"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, the goal is to generate new images using auto-encoders.\n",
    "The focus lies mainly on variational auto-encoders\n",
    "and how they differ from the classic auto-encoders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "Python version: 3.9.12\n",
      "Pytorch version: 1.13.0\n",
      "torchvision version: 0.14.0\n",
      "numpy version: 1.21.5\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torch import optim\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "torch.manual_seed(1806)\n",
    "torch.cuda.manual_seed(1806)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "\n",
    "\n",
    "print('Python version: 3.9.12')\n",
    "print('Pytorch version:', torch.__version__)\n",
    "print('torchvision version:', torchvision.__version__)\n",
    "print('numpy version:', np.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoMNIST(datasets.MNIST):\n",
    "    \"\"\" Wrapper around MNIST for training auto-encoders. \"\"\"\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        x = self.data[index].numpy()\n",
    "        img = Image.fromarray(x, mode='L')\n",
    "        target = Image.fromarray(x, mode='L')\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    @property\n",
    "    def raw_folder(self):\n",
    "        return os.path.join(self.root, 'MNIST', 'raw')\n",
    "\n",
    "    @property\n",
    "    def processed_folder(self):\n",
    "        return os.path.join(self.root, 'MNIST', 'processed')\n",
    "    \n",
    "    def get_subset_with_label(self, label: int):\n",
    "        indices = [i for i in range(len(self)) if self.targets[i] == label]\n",
    "        return Subset(self, indices)\n",
    "\n",
    "\n",
    "def get_auto_mnist_loader(train: bool = True, flat: bool = False, \n",
    "                          raw_targets: bool = False,download = False, **kwargs) -> DataLoader:\n",
    "    \"\"\"\n",
    "    Construct a data loader for training auto-encoders on MNIST.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    train : bool, optional\n",
    "        If `True`, use the MNIST training data, \n",
    "        otherwise use the test data.\n",
    "    flat : bool, optional\n",
    "        If `True`, the images is flattened in pre-processing.\n",
    "    raw_targets : bool, optional\n",
    "        If `True`, the target values are not normalised,\n",
    "        otherwise (the default), they are normalised\n",
    "        in the same way as the inputs are.\n",
    "    kwargs\n",
    "        Additional keyword arguments for data loader.\n",
    "        E.g. batch_size, shuffle, num_workers, ...\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    loader : DataLoader\n",
    "        A data loader that iterates over the MNIST data,\n",
    "        where the inputs are properly normalised images\n",
    "        and the targets are either the raw images with pixels in [0, 1]\n",
    "        or equivalent to the inputs, depending on `raw_targets`.\n",
    "    \"\"\"\n",
    "    to_tensor = transforms.ToTensor()\n",
    "    normalise = transforms.Compose([\n",
    "        to_tensor, transforms.Normalize((.1307, ), (.3081, ))\n",
    "    ])\n",
    "    \n",
    "    if flat:\n",
    "        flatten = nn.Flatten(start_dim=0)\n",
    "        to_tensor = transforms.Compose([to_tensor, flatten])\n",
    "        normalise.transforms.append(flatten)\n",
    "    \n",
    "    _transforms = {\n",
    "        'transform': normalise,\n",
    "        'target_transform': to_tensor if raw_targets else normalise\n",
    "    }\n",
    "    \n",
    "    data = AutoMNIST(\"~/.pytorch\", train=train, download = download, **_transforms)\n",
    "    loader = DataLoader(data, **kwargs)\n",
    "    return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_to_image(*data: torch.Tensor, \n",
    "                  means: tuple = (0, ), stds: tuple = (1., )) -> Image:\n",
    "    \"\"\"\n",
    "    Convert multiple tensors to one big image.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data0, data1, ... dataN : torch.Tensor\n",
    "        One or more tensors to be merged into a single image.\n",
    "    means : tuple or torch.Tensor, optional\n",
    "        Original mean of the image before normalisation.\n",
    "    stds : tuple or torch.Tensor, optional\n",
    "        Original standard deviation of the image before normalisation.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    image : Image\n",
    "        PIL image with all of the tensors next to each other.\n",
    "    \"\"\"\n",
    "    # concatenate all data\n",
    "    big_pic = torch.cat([x for x in data], dim=-1)\n",
    "    \n",
    "    means = torch.tensor(means)\n",
    "    stds = torch.tensor(stds)\n",
    "    to_image = transforms.Compose([\n",
    "        # inverts normalisation of image\n",
    "        transforms.Normalize(-means / stds, 1. / stds),\n",
    "        transforms.Lambda(lambda x: torch.clamp(x, 0, 1)),\n",
    "        transforms.ToPILImage()\n",
    "    ])\n",
    "    \n",
    "    return to_image(big_pic)\n",
    "\n",
    "\n",
    "def display_result(auto_encoder: nn.Module, xs: torch.Tensor, count: int = 10):\n",
    "    \"\"\"\n",
    "    Visualise a number of reconstructions from an auto-encoder.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    auto_encoder : nn.Module\n",
    "        The auto-encoder to visualise reconstructions for.\n",
    "    xs : torch.Tensor\n",
    "        Batch of images to be reconstructed.\n",
    "        The batch is assumed to have the correct shape\n",
    "        for feeding it to the network.\n",
    "    count : int, optional\n",
    "        Number of samples in the batch to visualise.\n",
    "\n",
    "    \"\"\"\n",
    "    xs = xs[:count]\n",
    "    model_dev = next(auto_encoder.parameters()).device\n",
    "    # get reconstructions\n",
    "    preds = auto_encoder.reconstruct(xs.to(model_dev)).cpu()\n",
    "    if preds.min() < 0:\n",
    "        preds = preds * .3081 + .1307    # unnormalise\n",
    "    \n",
    "    # convert to images\n",
    "    xs = xs.view(-1, 1, 28, 28)  # unflatten\n",
    "    x_im = data_to_image(*xs, means=(.1307, ), stds=(.3081, ))\n",
    "    preds = preds.view(-1, 1, 28, 28)\n",
    "    res_im = data_to_image(*preds)\n",
    "    \n",
    "    # paste together\n",
    "    im = Image.new('L', (len(xs) * 28, 56))\n",
    "    im.paste(x_im, (0, 0))\n",
    "    im.paste(res_im, (0, 28))\n",
    "    display(im, metadata={'width': '100%'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegularisedLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Loss function for networks that expose state for regularisation.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    To train an auto-encoder with some norm-regularisation (untested):\n",
    "    >>> ae = AutoEncoder(784)\n",
    "    >>> mse = RegularisedLoss(nn.MSELoss(), torch.norm)\n",
    "    >>> x = torch.randn(1, 784)\n",
    "    >>> logits, code = ae.forward(x)\n",
    "    >>> mse((logits, code), x)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, loss_func: nn.Module = None, reg_func: nn.Module = None,\n",
    "                 weight: float = 1.):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        loss_func : nn.Module\n",
    "            Standard loss function computed from logits and targets.\n",
    "        reg_func : nn.Module, optional\n",
    "            Regularisation function computed from exposed network state.\n",
    "        weight : float, optional\n",
    "            Weighting factor for the regularisation.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.loss_func = loss_func\n",
    "        self.regulariser = reg_func\n",
    "        self.weight = weight\n",
    "        self._regularisation_cache = []\n",
    "        \n",
    "    def split_loss(self, total_loss):\n",
    "        \"\"\"\n",
    "        Split total loss values in original loss and regularisation part.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        total_loss : list or float\n",
    "            One or more loss values computed with this function.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        orig_loss : Tensor\n",
    "            One ore more loss values from the original part of the loss.\n",
    "        reg_loss : Tensor\n",
    "            One or more loss values from the regularsiation part of the loss.\n",
    "        \"\"\"\n",
    "        if len(self._regularisation_cache) == 0:\n",
    "            raise ValueError(\"no regularisation loss founc, make sure to call forward first\")\n",
    "            \n",
    "        _totals = torch.tensor(total_loss)\n",
    "        reg_loss = torch.tensor(self._regularisation_cache[-_totals.numel():])\n",
    "        orig_loss = _totals - reg_loss\n",
    "        return orig_loss, reg_loss\n",
    "        \n",
    "    def forward(self, outputs, y):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        outputs : (Tensor, Tensor)\n",
    "            Tuple consisting of logits and exposed state.\n",
    "        y : Tensor\n",
    "            Targets for the original loss part.\n",
    "        \"\"\"\n",
    "        logits, state = outputs\n",
    "        loss = self.loss_func(logits, y)\n",
    "        \n",
    "        if self.regulariser is not None:\n",
    "            reg_loss = self.weight * self.regulariser(state)\n",
    "            self._regularisation_cache.append(reg_loss.item())\n",
    "            loss = loss + reg_loss\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def _forward(network: nn.Module, data: DataLoader, metric: callable):\n",
    "    device = next(network.parameters()).device\n",
    "    \n",
    "    for x, y in data:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        outputs = network(x)\n",
    "        res = metric(outputs, y)\n",
    "        yield res\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(network: nn.Module, data: DataLoader, metric: callable) -> list:\n",
    "    network.eval()\n",
    "\n",
    "    results = _forward(network, data, metric)\n",
    "    return [res.item() for res in results]\n",
    "\n",
    "\n",
    "@torch.enable_grad()\n",
    "def update(network: nn.Module, data: DataLoader, loss: nn.Module,\n",
    "           opt: optim.Optimizer, regulariser: nn.Module = None) -> list:\n",
    "    network.train()\n",
    "\n",
    "    errs = []\n",
    "    for err in _forward(network, data, loss):\n",
    "        errs.append(err.item())\n",
    "\n",
    "        opt.zero_grad()\n",
    "        err.backward()\n",
    "        opt.step()\n",
    "\n",
    "    return errs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def print_errs(epoch: int, errs : list, objective: nn.Module):\n",
    "    \"\"\"\n",
    "    Print error summary for current epoch.\n",
    "\n",
    "    If possible, the loss is additionally decomposed in\n",
    "    reconstruction and regularisation loss.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    epoch : int\n",
    "        Rank of the current epoch.\n",
    "    errs : list\n",
    "        The errors to summarise.\n",
    "    objective : nn.Module\n",
    "        The loss function that was used to compute the errors.\n",
    "        In case of a `RegularisedLoss`, this will be used to split\n",
    "        into reconstruction and regularisation parts of the loss.\n",
    "    \"\"\"\n",
    "    print(f\"Epoch {epoch: 2d} - avg loss: {sum(errs) / len(errs):.6f}\", end=' ')\n",
    "    try:\n",
    "        rec_errs, reg_errs = objective.split_loss(errs)\n",
    "        print(f\"(rec: {sum(rec_errs) / len(errs):.4f}, reg: {sum(reg_errs) / len(errs):.4f})\")\n",
    "    except (AttributeError, ValueError):\n",
    "        print()\n",
    "\n",
    "        \n",
    "def log_results(epoch: int, ae : nn.Module, vis_every: int):\n",
    "    \"\"\"\n",
    "    Sporadically visualise results.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    epoch : int\n",
    "        Rank of the current epoch.\n",
    "    ae : nn.Module\n",
    "        The auto_encoder to log distributions for.\n",
    "    vis_every : int\n",
    "        Frequency of visualising the reconstructions\n",
    "        and logging embeddings to tensorboard.\n",
    "    \"\"\"\n",
    "    dev = next(ae.parameters()).device\n",
    "    with torch.no_grad():\n",
    "        ref_inputs = torch.stack([loader.dataset[i][0] for i in range(64)])\n",
    "        z = ae.encode(ref_inputs.to(dev))\n",
    "\n",
    "    if epoch % vis_every == 0:\n",
    "        display_result(ae, ref_inputs)\n",
    "        ref_labels = [str(label.item()) for label in loader.dataset.targets[:64]]\n",
    "        ref_images = 1. - loader.dataset.data[:64].unsqueeze(1)\n",
    "\n",
    "\n",
    "def train_auto_encoder(auto_encoder: nn.Module, loader: DataLoader,\n",
    "                       objective: nn.Module, optimiser: optim.Optimizer,\n",
    "                       num_epochs: int = 10, vis_every: int = 5):\n",
    "    \"\"\"\n",
    "    Train an auto-encoder for a number of epochs.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    auto_encoder : nn.Module\n",
    "        The auto-encoder to train\n",
    "    loader : DataLoader\n",
    "        A data loader for iterating over batches of the data.\n",
    "    objective : nn.Module\n",
    "        The loss function to optimise during training.\n",
    "    optimiser : optim.Optimizer\n",
    "        The optimiser to use for training.\n",
    "    num_epochs : int, optional\n",
    "        Number of times to iterate the dataset.\n",
    "    vis_every : int, optional\n",
    "        Frequency, during training, of\n",
    "        intermediate visualisation of reconstructions.\n",
    "    \"\"\"\n",
    "    # evaluate random performance\n",
    "    errs = evaluate(auto_encoder, loader, objective)\n",
    "    print_errs(0, errs, objective)\n",
    "    log_results(0, auto_encoder, vis_every)\n",
    "\n",
    "    # train for some epochs\n",
    "    for i in range(1, num_epochs + 1):\n",
    "        errs = update(auto_encoder, loader, objective, optimiser)\n",
    "        print_errs(i, errs, objective)\n",
    "        log_results(i, auto_encoder, vis_every)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Variational Auto-Encoders\n",
    "\n",
    "Variational auto-encoders (VAEs) extend auto-encoders in a probabilistic way.\n",
    "Although this might sound complicated, it only requires a few modifications to make an auto-encoder variational.\n",
    "First of all, the latent space is regularised to stay close to a (standard normal) distribution.\n",
    "Secondly, VAEs do not produce specific codes, but rather a distribution of codes.\n",
    "In practice this is done by directly mapping inputs to the parameters of a distribution.\n",
    "\n",
    "The main advantage of this approach, is that the the distribution of the latent codes remains well under control.\n",
    "As a result, we can sample from this latent distribution and decode these sampled codes to generate new data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Task 1: VAE Architecture\n",
    "\n",
    "VAEs predict the distribution parameters of the latent space,\n",
    "rather than a specific code in the latent space.\n",
    "In the case of a Gaussian latent distribution, \n",
    "this means that the encoder produces a mean and log-variance for every input sample.\n",
    "By predicting the log-variance, the variance is guaranteed to be positive.\n",
    "\n",
    "Because the codes of a VAE are distributions, it is not possible to directly decode them back to images.\n",
    "Instead, specific codes have to be sampled from the latent distribution before decoding.\n",
    "This is the only real difference, architecturally, between AEs and VAEs.\n",
    "\n",
    " > Implement the `VariationalAutoEncoder` class.\n",
    " > The `sample` method should sample a latent vector from a Gaussian with given mean and log-variance. \n",
    " > Note that the `sample` function for a VAE must be differentiable!\n",
    " > The `generate` method should allow to create one or more new images, given a mean in the latent space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VariationalAutoEncoder(nn.Module):\n",
    "    \"\"\" Fully connected auto-encoder. \"\"\"\n",
    "\n",
    "    def __init__(self, in_features: int, code_features: int = 20, \n",
    "                 hidden_features: int = 400):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.code_features = code_features\n",
    "        \n",
    "        self.encoder_1 = torch.nn.Linear(in_features,hidden_features)\n",
    "        self.encoder_mean = torch.nn.Linear(hidden_features,code_features)\n",
    "        self.encoder_var = torch.nn.Linear(hidden_features,code_features)\n",
    "        \n",
    "        self.decoder_1 = torch.nn.Linear(code_features,hidden_features)\n",
    "        self.decoder_2 = torch.nn.Linear(hidden_features,in_features)\n",
    "\n",
    "        \n",
    "    def sample(self, mean: torch.Tensor, log_var: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Sample randomly from the latent space in a differentiable manner.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        mean : (N, code_featuers) Tensor\n",
    "            Mean(s) for sampling new images.\n",
    "        log_var : (N, code_features) Tensor\n",
    "            Logarithm of variance(s) for sampling new images.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        sample : (N, code_features) Tensor\n",
    "            The randomly generated sample in latent space.\n",
    "        \"\"\"\n",
    "        norm_distr = torch.distributions.multivariate_normal.MultivariateNormal(torch.zeros(self.code_features), torch.eye(self.code_features))\n",
    "        eps = norm_distr.sample()\n",
    "        std_norm = torch.exp(0.5 * log_var)\n",
    "        \n",
    "        z = mean + std_norm * eps\n",
    "        return z\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        #Encoder\n",
    "        x = self.encoder_1(x)\n",
    "        x = nn.ReLU()(x)\n",
    "        x_mean = self.encoder_mean(x)\n",
    "        x_logvar = self.encoder_var(x)\n",
    "        \n",
    "        #Latent distribution\n",
    "        z = self.sample(x_mean,x_logvar)\n",
    "        \n",
    "        #Decoder\n",
    "        x = self.decoder_1(z)\n",
    "        x = nn.ReLU()(x)\n",
    "        x = self.decoder_2(x)\n",
    "        return x,(x_mean,x_logvar)\n",
    "        \n",
    "        \n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def generate(self, mean: torch.Tensor, log_var: torch.Tensor = 0.,\n",
    "                 batch_size: int = 1) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Generate one or more new images.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        mean : (N, code_features) Tensor\n",
    "            Mean in latent space for generating images.\n",
    "        log_var : (N, code_features) Tensor, optional\n",
    "            Logarithm of variance(s) in latent space for generating images.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        img : (N, in_features) Tensor\n",
    "            Proper image with the same dimensions as the inputs.\n",
    "\n",
    "        See Also\n",
    "        --------\n",
    "        `sample(mean, log_var)` : sample randomly in latent space.\n",
    "        \"\"\"\n",
    "        #latent vector z\n",
    "        log_var = torch.as_tensor(log_var)\n",
    "        z = self.sample(mean,log_var)\n",
    "        \n",
    "        #decoder\n",
    "        x = self.decoder_1(z)\n",
    "        x = nn.ReLU()(x)\n",
    "        x = self.decoder_2(x)\n",
    "        return x\n",
    "        \n",
    "\n",
    "    @torch.no_grad()\n",
    "    def reconstruct(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\" Properly reconstruct some inputs. \"\"\"\n",
    "        x = self.encoder_1(x)\n",
    "        x = nn.ReLU()(x)\n",
    "        mean = self.encoder_mean(x)\n",
    "        log_var = self.encoder_var(x)\n",
    "        \n",
    "        \n",
    "        return self.generate(mean, log_var)\n",
    "    \n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def encode(self,x:torch.tensor) -> torch.Tensor:\n",
    "        x = self.encoder_1(x)\n",
    "        x = nn.ReLU()(x)\n",
    "        x_mean = self.encoder_mean(x)\n",
    "        x_logvar = self.encoder_var(x)\n",
    "        \n",
    "        return x_mean,x_logvar\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: VAE Regulariser \n",
    "\n",
    "The other main difference with auto-encoders is the regularisation\n",
    "to keep the latent space close to a standard normal distribution.\n",
    "\n",
    " > Analytically compute the Kullback-Leibler divergence\n",
    " > between a non-standard Gaussian distribution, $\\mathcal{N}(\\mu, \\sigma^2)$,\n",
    " > and the standard normal distribution, $Z = \\mathcal{N}(0, 1)$.\n",
    " > Implement your result in the `gauss_vae_regulariser` function.\n",
    " > Finally, use the `RegularisedLoss` to train the variational auto-encoder \n",
    " > with the regulariser on top of the reconstruction loss. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "$$\\begin{aligned}\n",
    "  D_\\mathrm{KL}\\big(\\mathcal{N}(\\mu, \\sigma^2) \\mathbin{||} Z = \\mathcal{N}(0, 1)\\big)\n",
    "\\end{aligned}$$\n",
    "\n",
    "probabilty density function of multivariate Normal distribution:\n",
    "\n",
    "$$p(\\mathbf{x}) = \\frac{1}{(2\\pi)^{k/2}|\\Sigma|^{1/2}} \\exp\\left(-\\frac{1}{2}(\\mathbf{x}-\\boldsymbol{\\mu})^T\\Sigma^{-1}(\\mathbf{x}-\\boldsymbol{\\mu})\\right)$$\n",
    "\n",
    "Now, let our two Normal distributions be $\\mathcal{N}(\\boldsymbol{\\mu_p},\\,\\Sigma_p)$ and $\\mathcal{N}(\\boldsymbol{\\mu_q},\\,\\Sigma_q)$\n",
    "\n",
    "\\begin{aligned}\n",
    "D_{KL}(p||q) & = \\mathbb{E}_p\\left[\\log(p) - \\log(q)\\right]\n",
    "\\newline\n",
    "& = \\mathbb{E}_p\\left[\\frac{1}{2}\\log\\frac{|\\Sigma_q|}{|\\Sigma_p|} - \\frac{1}{2}(\\mathbf{x}-\\boldsymbol{\\mu_p})^T\\Sigma_p^{-1}(\\mathbf{x}-\\boldsymbol{\\mu_p}) + \\frac{1}{2}(\\mathbf{x}-\\boldsymbol{\\mu_q})^T\\Sigma_q^{-1}(\\mathbf{x}-\\boldsymbol{\\mu_q})\\right]\n",
    "\\newline\n",
    "& = \\frac{1}{2}\\mathbb{E}_p\\left[\\log\\frac{|\\Sigma_q|}{|\\Sigma_p|}\\right] - \\frac{1}{2}\\mathbb{E}_p\\left[(\\mathbf{x}-\\boldsymbol{\\mu_p})^T\\Sigma_p^{-1}(\\mathbf{x}-\\boldsymbol{\\mu_p})\\right] + \\frac{1}{2}\\mathbb{E}_p\\left[(\\mathbf{x}-\\boldsymbol{\\mu_q})^T\\Sigma_q^{-1}(\\mathbf{x}-\\boldsymbol{\\mu_q})\\right]\n",
    "\\newline\n",
    "& = \\frac{1}{2}\\log\\frac{|\\Sigma_q|}{|\\Sigma_p|} - \\frac{1}{2}\\mathbb{E}_p\\left[(\\mathbf{x}-\\boldsymbol{\\mu_p})^T\\Sigma_p^{-1}(\\mathbf{x}-\\boldsymbol{\\mu_p})\\right] + \\frac{1}{2}\\mathbb{E}_p\\left[(\\mathbf{x}-\\boldsymbol{\\mu_q})^T\\Sigma_q^{-1}(\\mathbf{x}-\\boldsymbol{\\mu_q})\\right] \n",
    "\\end{aligned}\n",
    "\n",
    "\n",
    "**second term:** can be written as $tr\\left\\{(\\mathbf{x}-\\boldsymbol{\\mu_p})^T\\Sigma_p^{-1}(\\mathbf{x}-\\boldsymbol{\\mu_p})\\right\\}$, where $tr$ is a trace. Using trace property it can be rewritten as $tr\\left\\{(\\mathbf{x}-\\boldsymbol{\\mu_p})(\\mathbf{x}-\\boldsymbol{\\mu_p})^T\\Sigma_p^{-1}\\right\\}$\n",
    "\n",
    "\n",
    "**second term:** $ \\frac{1}{2}\\mathbb{E}_p\\left[tr\\left\\{(\\mathbf{x}-\\boldsymbol{\\mu_p})(\\mathbf{x}-\\boldsymbol{\\mu_p})^T\\Sigma_p^{-1}\\right\\}\\right] = \\frac{1}{2}tr\\left\\{\\mathbb{E}_p\\left[(\\mathbf{x}-\\boldsymbol{\\mu_p})(\\mathbf{x}-\\boldsymbol{\\mu_p})^T\\Sigma_p^{-1}\\right]\\right\\} = \\frac{1}{2}tr\\left\\{\\mathbb{E}_p\\left[(\\mathbf{x}-\\boldsymbol{\\mu_p})(\\mathbf{x}-\\boldsymbol{\\mu_p})^T\\right]\\Sigma_p^{-1}\\right\\}$\n",
    "\n",
    "using definition of covariance we get:\n",
    "\n",
    "\\begin{aligned}\n",
    "& = \\frac{1}{2}tr\\left\\{\\Sigma_p\\Sigma_p^{-1}\\right\\}\n",
    "\\newline\n",
    "& = \\frac{1}{2}tr\\left\\{I_k\\right\\}\n",
    "\\newline\n",
    "& = \\frac{k}{2}\n",
    "\\end{aligned}\n",
    "\n",
    "**third term:** $\\mathbb{E}_p\\left[(\\mathbf{x}-\\boldsymbol{\\mu_q})^T\\Sigma_q^{-1}(\\mathbf{x}-\\boldsymbol{\\mu_q})\\right] = (\\boldsymbol{\\mu_p}-\\boldsymbol{\\mu_q})^T\\Sigma_q^{-1}(\\boldsymbol{\\mu_p}-\\boldsymbol{\\mu_q}) + tr\\left\\{\\Sigma_q^{-1}\\Sigma_p\\right\\}$\n",
    "\n",
    "\n",
    "**all together:** $D_{KL}(p||q) = \\frac{1}{2}\\left[\\log\\frac{|\\Sigma_q|}{|\\Sigma_p|} - k + (\\boldsymbol{\\mu_p}-\\boldsymbol{\\mu_q})^T\\Sigma_q^{-1}(\\boldsymbol{\\mu_p}-\\boldsymbol{\\mu_q}) + tr\\left\\{\\Sigma_q^{-1}\\Sigma_p\\right\\}\\right]$\n",
    "\n",
    "\n",
    "**If $q$ is $\\mathcal{N}(0,\\,I)$ then:**\n",
    "\n",
    "$D_{KL}(p||q) = \\frac{1}{2}\\left[\\boldsymbol{\\mu_p}^T\\boldsymbol{\\mu_p} + tr\\left\\{\\Sigma_p\\right\\} - k - \\log|\\Sigma_p|\\right]$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gauss_vae_regulariser(parameters):\n",
    "    \"\"\"\n",
    "    Compute the regularisation factor for training a VAE\n",
    "    with a Gaussian latent space.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    parameters : (Tensor, Tensor)\n",
    "        Mean and log_var predictions from variational encoder.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    kl_div : Tensor\n",
    "        The Kullback-Leibler divergence between a Gaussian distribution\n",
    "        with the given parameters and the standard normal distribution.\n",
    "    \"\"\"\n",
    "    mean_pred = parameters[0]\n",
    "    log_var_pred = parameters[1]\n",
    "    D_KL = - 0.5 * torch.sum((1 + log_var_pred - torch.exp(log_var_pred) - torch.square(mean_pred)))\n",
    "    return D_KL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0 - avg loss: 0.698485 (rec: 0.6982, reg: 0.0003)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=280x56 at 0x7FC4A926CFA0>"
      ]
     },
     "metadata": {
      "width": "100%"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1 - avg loss: 0.143045 (rec: 0.1289, reg: 0.0141)\n",
      "Epoch  2 - avg loss: 0.116881 (rec: 0.1030, reg: 0.0139)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAARgAAAA4CAAAAAADPrjSAAAMK0lEQVR4nO1bfXzO5Rr/Ou0QmdYiO2RRpOZtSuEcHPkoOUTS5GV1hlNIyktehpGDT5SXeTcRHUVbiklGbFlRhGxL8tJo87J5GXvB5Lmu5zp/PNuePc993c/K59TpfM6+//jte93XfV/P9bt/93Xd930BylGOcpTj94EHV/LK5v9tIwBgnqQF/3q9JyYZ1E2BgYFRM9bXWiOFk0yF0ItElOOryw7ZDS2SCSztfrGFVYMGRlZU+Lo5TJ0sOveGDBZmZv5I0fRrt7PsUecWxnj8HVz/uWVxzMycsY7zv2hvKDx8kunSWWpVasC2PTybjN1kcUzEZQe3LdsmD9RbkkJE8xVJlfU2x4S8eSKTmIiI6G1/Q1zdeSaorGFnFOaHlf67+UUugiP8qadaGj+wcpsTxPT108SRbjJqtUebCjGpFsdMcqiOabkwlXl4r4UtTVHDpZeJT6SROgejbY7ZSETFjqG/GOLqTmeobqAbO2ibx9+Bx5iZedfmwjy1/WrXiBGJvNZN/uDpmNrOf+mDdTzvSAuuZNC9sokTU4n4fW9JtaWXiOhwcAOiNmZ3AZ/ZHDOM6My06dOmJdocI6G6hUDbrYEA0Of8kWaegu7LX2TeXwUhyzS1By8yJ43gU826S6lfcdzTMQnOieqYbU45HM8ZrF+rfEp6xK/qZuJR3rIIIqIjdaA7plY60wR19fWrUycIAPwzidaZi0x1p7OVaiGAw9wGAA5KD2+Jf4Vl3NeiFXqR6OOqXcbVALigJDA1veLpmF1O5ZsA8BZzoslGECX4A+FEGTW8ZZ8Q/bA2GHhCdQwmMtFQi6kAgLAComiTru50WtW+oY4AQvO5jyl7k5MqqEr3vsdnU552PTO9V0yPdXo4pmaWs46mXZ0d5zoY7DSm+f4AvifqbghrvfbnGgAwUHcMynBM70QiMtdeBFxyzrWoTHUcrAFUWUs7/UxhlSR+TFOquJFyO91eu9imL4r5lc6xpZutdh4OULTr7mdHlMFGceGGykClbld4isVWACssjhFmu2P6HSwkon2VFdFGm2PqZBe2AxBDmar4nryMVUPNSdOKyJ2EeDjGPRH8wzYWOvtpnQ528JZq3mRANm0AUH8PUWwV3daXx0WO+5o+v0mT2WZM3YnJyclMRBefr63JbY5pcozmAhh1jQbpxvTIZR5jhPpdXCoXFC5JklY6wwA0az4qenFewbmP80gLrd1zHck1DbYGUXCNMTvzmBxPaIZUfuhjZmE+dY9qp8UxTY4Xh+t4VQ0b1bjpF8HCuyMrBu25/rauBzT5lHmxl7O7XqVXStu0sPhxMeccOHCAnddzd83tV9vv7HWlw7rMvNKkA7KIiSjzJGVplj58kgpOxeUTZY3WUl+7Y5iZhZm5s6aGjc5chQ0n4iNEu3Vjii0OJ/bMcBBG7nyx4uv8adUSyZj4+Pj4+AGuWPS88welvyUOh0ObSC3P8ZE3QoJ2kDK3K3YjmvgXBKYQEfXSPCPMcZr1weNbNG7cuPFcIt0xwzXH9HIUZj0SmkjE5Dipz1AAwDW+1t6DCKPjJSZPpQzbLiXWOdMkQ9MdjnX2sdoKDzNIv9eJNgWgxl4unPIh0ZYOzY2dKxNRiL3bajbH9HReMfOfpPQBAEJ2EhNZMlSg6T8TmA94rr9hNK/oKfQ9+tBqTayWPJ1zOHZWNelidGIycpibZlDekNvw0G46/Aj8H1+dR+4XU4xFpOYpbostjunuvHqvQb5SBwDaXqKwkBAlxgNAw4Wnmfn6Zq9hOMP1MOIiWz1qcQw7HErCVEpuOmYw5fcO7Bx3maJcWVGfTZsaeLcZpjnGr0tRhB6Qb3MMDjkX64JqC/mozcigEenMzHu6efFhdG1+aJ2wjRl8fK2e2gIAYsVM+1cKs6+TE23GZNGV/YeJaIIaqItwlFm8loO2CVQHAALDLxEVPKLrReeZWzYAwDjK8ow5JXlezUYL7gOwe9YGMbT8XuyZ3wD4Kkk5pXFD/uDNhD7qvL7orA+VuxUuu0alZticHP8j+1D87m44vagFjTGmAMCjDwh2LPnMZqQWOoHg52XZaU0Q+MExZuYvuiv5Yu2viJjo7DxTVBqxzhhvqr2DtUjlRhNhY8ZUDZ87rqYapEuhMxF7zZiUotMG4jMxlmkBRDufUvmjtEqjW67LZGa+PF1PQoNeI6bZxofuhVj55Y7BUfLxcfpAcJrhmOYriIjoyIH5Tex6Zwr1E6NIUh02g5kPvj4t4IZsLEaEOWOCkstyTAQl+gi7vwwVB52jdYN8H9G9n/orHhb/J+G/heIse6X/c/gv8JWplaMc5ShHOcphRe3HfhdX1wiZ8sBvP+h9D3e0/foxIl+Yx/0/C/4fqSeYN4gPJcUufGDae6cv7xyrygJrld35lPl3eFPTd23LERH5LlE9jDooIjLBV5/1rBKRNWWb5IngOdviYpeqiexRMTe6RWiwSVwoNDZhAD7dXva4X0Z7EVX3uno8mRTZRWnfRESubI8e+oyPPs0LoiIkiKjH7+9KdswL+i3fZhGRn36cqskOiVi2ia0KROTLWbGZIiNMaaDdoSVotP5JL2bhJZHMSa0tb/0hEVl+NxpOHm8cLrgxqYXO3y+i7enb5YmM7Drj0BuKLFNE1g2ctV297hgtYtl7LpL0wQDQVOQDU/rHn+GY6TO8mZ4xMUPta6uIDACAR2eW2mV38LwXeSihta68RApuM9lBp2UigLBM5VC4n8ieW4DA0WGmDHhGZKQ+Urshrn+nirnXByBi3HAVI3QyAKD/cWVdv9O+XxeRoiWp10A3++7jHo3ijlm1XzDJPpmuw/N5stAUthr2IAA0iO6q9ecv8pHVVABoLvrcyJI7bSo5cQBQ7eAOTdjeduaSIFL8yru/07GEHuFxnttXknXtY3LVJEP2rQAAzPI1vdekPa3yP/r+JgaISHtNsFUaW1RS5G8AsDqjmSZ9cmuyel4TJFJyjDHkVMmlPqZOK91qk+jn3pNVK2cnAAB6irxqsRV4W46pdmK3iHodDAD4xzWR2FtV0Rzpr+sMlVUA8JJoFVxAP5ZD2vF61Hl3OdQcWVTyPGNwqUb1936ndtpMJN1k6+2LBoD+ljkPAOFp6goKAKkio3XJqFwRkVOWgr/Zsl7l7zgU7wcgVNICdMXRp65vUzKHT15yP++Tks/+5mdDAKDhhryz2ZF/6p++Qe0zR/3p7U8kt2rR+x0RUcsZgG5viUjOeF24W0T/xma6Uo4jf9X1puuZ4f0nZf9of7wqMk7XA+4YtmHbUm+y56elotW13JLHZrcCqNVjuYhsaVd50vnpWo+LRHpq/IKU3dcunFh1VL8ubjQ8fvcJkc8H6YtlroieM71ZlN7pEwPvS4bC1j8mIpKSKpIbqusBwIBLEu1FjUxwP78lvUqei28U+ncBgB7fanWZIVYj0eKZJtU7HVcr225pPnjOioM/blszYdhdmjxL5HW900o3AzX3yDe6dLQ6Y5ptOnoqOzVJRGwVZQDQSMTrUh8z3RMsUqyVsg+u0pLw077jxxAxa4pc6Lhq/ZZ5iyL7qpemySJ7ffTaQbL1+5da6oxxoWu+GLWApTHo3FnvMLGkJFStzf3Wqnj7KoV8TsSyTLjwtliqBVvEX0wPt94uTfCxZgNoIRe0lwQgj6xKfz87x0eXeDFtu3dOVW9j0SIxXuRNu2b7g+1N8phc8DUaph4yNz0B7dp2HrqKM3zcOFVKFQm3i9dZd9/fKgHShca7PMt4PErxege3rpX98mEvlcJ7un0IYPZIOBsYdQdu3HVSeRsBUGpDSqHKmZ8M7oHmFSo1rj71NR9qP33fVL3edWFOTxgbniJ838imFHerWZBSjJc/u5g4ROH3y+dRy0VEXSdLMFw5AumctdWnDkYo24HWr6zZMN6y6yrGAJEdBulK6oYViMyy6U21RYLtovuy+t19JkbvShijCue7QmBdn6aiU1OTW2tEOC8s2KCQPQaZRd3eEPGe2A2izsvpb3NEJFuNZACAyRf0NS1iU4RGtxy45pvM9LXWWoZnt2Vm+vikXbhF4Q7IFt9KC2aX1a0Fw50ve1MjXO8vx9f/5hiZatlFWtb5++cuHHeDh5Y+MSDmPt8N5kQp9cY3inujUw/HWMJRMawl897QS8F/KwT1vWrk2eUox/8k/g03QeF8JlF7wwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=280x56 at 0x7FC4A926CFA0>"
      ]
     },
     "metadata": {
      "width": "100%"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  3 - avg loss: 0.113071 (rec: 0.0993, reg: 0.0138)\n",
      "Epoch  4 - avg loss: 0.111086 (rec: 0.0973, reg: 0.0137)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=280x56 at 0x7FC4A926CFA0>"
      ]
     },
     "metadata": {
      "width": "100%"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "loader = get_auto_mnist_loader(flat=True, raw_targets=True, batch_size=32, shuffle=True, num_workers=0,download = True)\n",
    "var_ae = VariationalAutoEncoder(784).to(device)\n",
    "adam = optim.Adam(var_ae.parameters())\n",
    "bce = RegularisedLoss(nn.BCEWithLogitsLoss(), gauss_vae_regulariser, weight= 1e-5)#nn.BCEWithLogitsLoss()  nn.MSELoss()\n",
    "train_auto_encoder(var_ae, loader, bce, adam,num_epochs = 4, vis_every = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAMQAAAAcCAAAAAAboE/1AAABiElEQVR4nO2Yr04DQRCHf1MOBAqBLQpXgisOQS0JBssTYNAkTZM6gkM2KN6ABEtCJapvgDwNCbbJYK7t3fxZLk2ZI03HNNlv7ma+3bvbbImxXNCS1/1FtJpuYBWxkfgvsX4SVAormYjoyLnRM3O+0xTjImQSi5CknLs7SxrrYiGsnsMC9BwpZuZrVS6IaQm5BnPazQ611vl8ZE+Vi2LZrHOq/gr5/uDu48kiOAUAtMw9M4xVp9ZYhSJeBSvdYqKfxmAm+nIlgEdPAomCMayuxLFg8Z0mmG7MXYorVwL5vl8xgBnT61r0XYnO+M0tGMBkowmLT+DMkcCWHgpkarp9Cb4ByJGANRbFSGURAOfS7ekCyf2EjbEollVR8rz2NQVGDnsBMGiM6TfCe5yGAC6dd+KdmdtOvQCmFWyHDgAMjWwc3JsvSSTTTRkGPU1kulkviv2u8N22SJE+SZQLY1T9AlhXkknKo/bnII5Vjqem/QOAW2deToD8wj7KRjK9T9SMzZ9nK461kPgBxLfuFRCj1XUAAAAASUVORK5CYII=",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=196x28 at 0x7FC4986BD850>"
      ]
     },
     "metadata": {
      "width": "100%"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAMQAAAAcCAAAAAAboE/1AAACu0lEQVR4nO2WX2hOcRjHv0nNtO1iplaaXLhC5s+NWu0ftqKIlERxNVrNUswlkTdmWIrmQtnULlxwRS+RmqTRcCM3ckOKTahJRvu4OOe8eXee5/fObuzifW7O6fmc7/N8n9/vvL/zSsUoxuyKm8D/9jCNIGSTIPXjMQ2LZuSmLEiBJicf8DmzGaKaHHbpfU85HGznWg0OEZxhZHzikwmy8RA7LdhQqJ9dM6HvXbI3IJsXqAhI21KeIrLLlJ0vtO/8ClEXnfBU+hxeMyokDaRXtoBPtx1AWx37bVoSVLpIQKlNlgONuc7U5cGtwXbL3MbRgDUXZmK0PbntYbVV1VbFCCC9dj3ljq48qTjXqH21X5LUPjUfN3JKRrA2uhmGJymWZPakCP3x1apesmNNc9fv60a7oWjg3YFXbsDcigJDtEiSRo2nkhTUpwhUnPRPtn0emZ+TBzZ50ExH1ypLF1c7Y7HogMlApSWLYtJzEli6LQV++s/MtCSp3hTGXlqsejVS1W34tthgp/H3QZmCX9CpW5tHJ+y09DD8tftgkIUw8hXoC5ihyyW2qDq8SVLrK1jv2YylPzZNobXuiiYy90Pi+gFYGd0O/bSK+jNI6jYfiM2USusMPApAWyrflRu+2ul2A4CzaZCBl9HdNTiWh94EZziVlVTmPQFclgSrUmgAYBy4m5/vzQ3Bu1azY19lxrGUHcy15XnKyVJJUkc2rfsI0SNrnSmumHnpaVPSLj9fyd9hVby4OfxqmMpJVsTXEVMibQjsRKPTqu/oOTU/MO0syfhDBMYL4yO9kuqDx4w/hDODHoWEYzHb+A/tMtubuxNqHN3lty75Pr9HsjsWOw6HvCHCKyotuEfGSH9xVR25XThoFuwMtgN44RFPpNfBmn4AdNpoDBieE1SG6r51yIHpm5tuAN6f1WIUY1bGH3g43e0aJSXuAAAAAElFTkSuQmCC",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=196x28 at 0x7FC4988DD9D0>"
      ]
     },
     "metadata": {
      "width": "100%"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# sanity check: generating new images\n",
    "var_ae.cpu()\n",
    "five_orig = 1 - loader.dataset.data[0].float().unsqueeze(0)\n",
    "five_mean, five_log_var = var_ae.encode(loader.dataset[0][0])\n",
    "five_recon = 1 - var_ae.generate(five_mean, five_log_var).view(1, 28, 28)\n",
    "five_new = var_ae.generate(five_mean.expand(5, -1), five_log_var.expand(5, -1)).view(-1, 1, 28, 28)\n",
    "\n",
    "im = data_to_image(five_orig, five_recon, *five_new)\n",
    "display(im, metadata={'width': '100%'})\n",
    "\n",
    "rand_new = var_ae.generate(torch.randn(7, var_ae.code_features)).view(-1, 1, 28, 28)\n",
    "im = data_to_image(*rand_new)\n",
    "display(im, metadata={'width': '100%'})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Task 3: Control of Generation\n",
    "\n",
    "One of the main virtues of VAEs is that they can be very effective\n",
    "in manipulating existing images to create variations of these images.\n",
    "E.g. non-trivial interpolation or transformations of images\n",
    "can easily be done by doing some arithmetic in the latent space.\n",
    "\n",
    " > Use a VAE to implement the generative functions below.\n",
    " > The `interpolate` function should be able to generate images\n",
    " > that are *visually between* two given images.\n",
    " > The `transform` function takes two reference images\n",
    " > that *describe* some sort of transformation\n",
    " > together with an image that should be transformed in the same way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate(vae: nn.Module, start_x: torch.Tensor, stop_x: torch.Tensor, \n",
    "                count: int = 1) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Generate interpolating images with a VAE.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    vae : nn.Module\n",
    "        The variational auto-encoder.\n",
    "        The module should have an `encode` and `generate` function.\n",
    "    start_x : (1, in_shape) torch.Tensor\n",
    "        The image to start interpolation from.\n",
    "    stop_x : (1, in_shape) torch.Tensor\n",
    "        The image to stop interpolation at.\n",
    "    count : int, optional\n",
    "        The number of images to generate between the original images.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    interpolated : (`count + 2`, out_shape) torch.Tensor\n",
    "        The interpolated images together with the original reconstructions.\n",
    "    \"\"\"\n",
    "    images = [start_x]\n",
    "    \n",
    "    alphas = np.linspace(0,1,count + 2)\n",
    "    alphas = alphas[1:len(alphas) - 1]\n",
    "    \n",
    "    start_mean,start_var = vae.encode(start_x)\n",
    "    stop_mean,stop_var = vae.encode(stop_x)\n",
    "    \n",
    "    new_means = []\n",
    "    new_vars = []\n",
    "    for a in alphas:\n",
    "        new_means.append(start_mean * (1-a) + stop_mean * a)\n",
    "        new_vars.append(start_var * (1-a) + stop_var * a)\n",
    "        \n",
    "        \n",
    "    for i in range(count):\n",
    "        images.append(vae.generate(new_means[i],new_vars[i]))\n",
    "        \n",
    "    images.append(stop_x)\n",
    "    return torch.cat(images,axis = -1)\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIwAAAAcCAAAAACWzQihAAACVUlEQVR4nO2Wu2sUURSHzwYCkZBgI6JFrAwINnYGC2MI6OI0PgKihYr4wNiqGHykELYSFERxsVG0cK3Uwhe4TSxUVNSYP0A0PlAR/4D9LGZn5pz7GFbQVHuqc+53Hr+7d+/MiHSta39nFRPhrSyo9SgfREQaobSBKwAvgy32zwHsDfcfrzF7b3eYVb/AnjACEGmQSrI29ZHUWjs9drqNfqwPNL3ThoGekiHOhVCSpzjsWWtElo7fPnL0kM/WUthht+mJgi1x2Q1V6GvReg2bL8JFMOcW/hJZuTmtfOd3jQ1MiEIVJyYSkWEuqugWyyOFsgNGPS2t5mSPyFdPDVExSVxb9r/ObQabuUJiqX0Az+siIrIlIkbkmismcGpJ4X92mLpvd23lI5oqegrUTraDMX4GtaQHNlMmRq+wNYrEve6mETBYRN80SszhANXOxFx32Soj5kypGBVd4rxBertlfwsRaeQr39nlCtWFmwyqq/g+XFDogB2ooycdi4HfrpjVmbcBlhk0ykM90JZFxWSp2esgkZhNy4C7tCZzmiLzhlRkLNrIsQdRErhOyn3lMJ1mCy9zTLPtsRnhXybS1IpxmambMUix9yUjAHqj40sOuO6mPi78kY53uBH6rRj6dORMV7F9Ir/V7KCtmwWOR3Z0U8XD/ubzlVPBV2X72ep9RkwBbwZFRGrAhFfGusyreOx17vnj0mfdPl9LqiHyipWzBRpyWUnZUAmjhKmE4LeeVD8AeNdc9Q19CE622VUfbcu1vAgO/C/WO7E4Qvo/ATC9cFq69g/sD+nrd5IUPZI2AAAAAElFTkSuQmCC",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=140x28 at 0x7FC49885DAC0>"
      ]
     },
     "metadata": {
      "width": "100%"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# sanity check\n",
    "thin_zero, wide_zero = 68, 56  # MNIST indices\n",
    "start, _ = loader.dataset[thin_zero]\n",
    "stop, _ = loader.dataset[wide_zero]\n",
    "\n",
    "interpolations = interpolate(var_ae, start.unsqueeze(0), stop.unsqueeze(0), count=3)\n",
    "images = interpolations.view(-1, 1, 28, 28)\n",
    "display(data_to_image(*images), metadata={'width': '100%'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(vae: nn.Module, start_ref: torch.Tensor, stop_ref: torch.Tensor,\n",
    "              x: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Transform an image based on the transformation between two other images.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    vae : nn.Module\n",
    "        The variational auto-encoder.\n",
    "        The module should have an `encode` and `generate` function.\n",
    "    start_ref : (1, in_shape) torch.Tensor\n",
    "        The reference image before the transformation.\n",
    "    stop_ref : (1, in_shape) torch.Tensor\n",
    "        The reference image after the transformation.\n",
    "    x : (N, in_shape) torch.Tensor\n",
    "        The image(s) to transform in the same way as the reference.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    transformed : (N, out_shape) torch.Tensor\n",
    "        The images in `x` transformed according to the reference.\n",
    "    \"\"\"\n",
    "    start_mean,start_var = vae.encode(start_ref)\n",
    "    stop_mean,stop_var = vae.encode(stop_ref)\n",
    "    \n",
    "    diff_mean,diff_var = (stop_mean - start_mean),(stop_var - start_var)\n",
    "    \n",
    "    mean,var = vae.encode(x)\n",
    "    mean += diff_mean\n",
    "    var += diff_var\n",
    "    \n",
    "    x_img = vae.generate(mean,var)\n",
    "    \n",
    "    return x_img\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=280x28 at 0x7FC49885DF70>"
      ]
     },
     "metadata": {
      "width": "100%"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAARgAAAAcCAAAAACfA//yAAADIklEQVR4nO1YO2gUURS9qIWCELBRG1dE0EIsYqFYKIiiTUKwENFWBV0Qg6IkYiFqY8QPLCSIBoJsUihrF0JIiGlTCFolKoKCoFEUg58ommMx82ZnZ+65b2cgVnObGd6559zz7nuz7yUiRRRRRBFFpGLV/y4IlLJS7s0BGF0MMzTuAMhBW7o5b0GAVQQmdMpdhPFQhR+P5/ViBABM5WLhzZNvPKNv2mDqnemkSwQYtBBZbhru2s8QIgsAt1abZhTsKAD0yQe68nRXRKodOmaYBwAMZjMaoE+fmS3VwGBw2CTp7RwOHszLqM+KrvpK0xMB0M3WtubdTe3EaUTsVjERmf2TbQpukJuxrDAmAEzsVgRFALQKMTPmyin1DvvqGQ0VEdmqz+F++HyrSbJyDrQwkT1AjYI6j01iKBis8DXS3bQDwA1eUERk8oiG7XIpnxVaLXhyTWun6XA0dDJFnHd9OZeuN2YcqsB3qxwl1vuZxgabmEK/0nBLMyalqjKfAmDEwDj0hSZZfYnSa+rqGoXx0qc5hB0aFEl1KI2hmtbkDWwcOESTLMn65D8pNgeipB9UYElqZKPbMCu0cvUq5Sy3yvVNNeZ0ArgEYEEp7UbO8ILxnW0kHaPY3/RBF/7CzKpKUN+9Ed9qO5MXOYe1JSVbAEwnkupBL3xBevpNMdWMQOMQvd+41xLwSMlopR4i5oFcTjOtQ1ONqQK/tfEtIe16EnixUGEu7N9eIUeuNDYm3xLqWG/6wE0U+cg0y6SaGyUHtsfda9q644zo8h8AJ3R1T2P6VcJtlu7R7AVw1SZqNP5FhA/eOsqccXvUuAS+I5C0AXsTQwPPASyjWg2O1WL6hrnpIVI1fgsXEakA2ylzPngezGZV97kGAK4Z+SWVFhds0cZ/bnK4Aq407JltEaGL6OupLavM8Kt6bsYIlzf0AKe4nl5tZGabiPxS4SvmvO1/w/HZvTf6MgcAOGvKJnn0qwvjPPYZKICLvJRuc629dJ2Wmynzy5y0Pnjzk1bgqpXvC6uetat9q8GjKx+tyv9CERGRdReSI+V8hcLwLUQRRSxO/ANJ8ZiRpvTAWQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=280x28 at 0x7FC4A8547B50>"
      ]
     },
     "metadata": {
      "width": "100%"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# sanity check\n",
    "thin_indices = 1, 14, 76, 44, 2, 35, 18, 84, 144, 22\n",
    "digits = torch.stack([loader.dataset[i][0] for i in thin_indices])\n",
    "ref_im = data_to_image(*digits.view(-1, 1, 28, 28), means=(.1307, ), stds=(.3081, ))\n",
    "display(ref_im, metadata={'width': '100%'})\n",
    "\n",
    "start_ref, _ = loader.dataset[thin_zero]\n",
    "stop_ref, _ = loader.dataset[wide_zero]\n",
    "\n",
    "transformed = transform(var_ae, start_ref.unsqueeze(0), stop_ref.unsqueeze(0), digits)\n",
    "images = transformed.view(-1, 1, 28, 28)\n",
    "display(data_to_image(*images), metadata={'width': '100%'})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
